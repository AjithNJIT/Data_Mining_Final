{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AjithNJIT/Data_Mining_Final/blob/main/Data_Mining_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "                **CS634- DATA MINING -AC2449**\n",
        "\n",
        "                **SEMANTIC SEARCH ANALYSIS USING SBERT**\n",
        "\n",
        "**This Project covers the Semantic Search using SBERT on Youtube videos dataset. It **covers** the Pytorch implementation of nearest neighbours search. We have used the Approximate Knn with the help of an multilingual embeddings model that was **used** for quora **data** **analysis**.\n",
        "\n",
        "**We** **have** used **Fox** **News** **channel** Id to Pull all **the** video links. The **Video** **Id** **link** that was retrived is **massaged** to **fetch** the video id from the entire link\n",
        "\n",
        "Example :\n",
        "https://www.youtube.com/watch?v=UyXJva9vg9o\n",
        "After Massaging:\n",
        "UyXJva9vg9o "
      ],
      "metadata": {
        "id": "ExN6RwOteVqC"
      },
      "id": "ExN6RwOteVqC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7c1deba",
      "metadata": {
        "id": "c7c1deba"
      },
      "outputs": [],
      "source": [
        "\n",
        "import scrapetube\n",
        "import sys\n",
        "\n",
        "path = '_list.txt'\n",
        "\n",
        "print('**********************\\n')\n",
        "print(\"The result will be saved in '_list.txt' file.\")\n",
        "print(\"Enter Channel ID:\")\n",
        "\n",
        "# Prints the output in the console and into the '_list.txt' file.\n",
        "class Logger:\n",
        " \n",
        "    def __init__(self, filename):\n",
        "        self.console = sys.stdout\n",
        "        self.file = open(filename, 'w')\n",
        " \n",
        "    def write(self, message):\n",
        "        self.console.write(message)\n",
        "        self.file.write(message)\n",
        " \n",
        "    def flush(self):\n",
        "        self.console.flush()\n",
        "        self.file.flush()\n",
        "\n",
        "sys.stdout = Logger(path)\n",
        "\n",
        "# Strip the: \"https://www.youtube.com/channel/\"\n",
        "channel_id_input = input()\n",
        "channel_id = channel_id_input.strip(\"https://www.youtube.com/channel/\")\n",
        "\n",
        "videos = scrapetube.get_channel(channel_id)\n",
        "\n",
        "for video in videos:\n",
        "    print(\"https://www.youtube.com/watch?v=\"+str(video['videoId']))\n",
        "#    print(video['videoId'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Once the Data(Video) is Fetched , then We have run a api to retrive the Title and youtube link inorder to feed it to the SBERT Model to create Embeddings**\n"
      ],
      "metadata": {
        "id": "l0iBCj99f3e0"
      },
      "id": "l0iBCj99f3e0"
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Iterable, Tuple, Dict, List, Any\n",
        "\n",
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "\n",
        "import urllib.request\n",
        "import json\n",
        "import urllib\n",
        "\n",
        "\n",
        "def fetch_transcript(vid_id) -> Tuple[str, List[Dict[str, Any]]]:\n",
        "    params = {\"format\": \"json\", \"url\": f\"https://www.youtube.com/watch?v={vid_id}\"}\n",
        "    #print(params)\n",
        "    url = \"https://www.youtube.com/oembed\"\n",
        "    #print(f\"Video id {vid_id}\")\n",
        "\n",
        "    query_string = urllib.parse.urlencode(params)\n",
        "    url = url + \"?\" + query_string\n",
        "\n",
        "    #print(f\"Fetching from {url}\")\n",
        "\n",
        "    with urllib.request.urlopen(url) as response:\n",
        "        response_text = response.read()\n",
        "        title = json.loads(response_text.decode())[\"title\"]\n",
        "\n",
        "    # retrieve the available transcripts\n",
        "    transcript_list = YouTubeTranscriptApi.list_transcripts(vid_id)\n",
        "    \n",
        "    utube_url = f\"https://www.youtube.com/watch?v={vid_id}\"\n",
        "    #print(utube_url)\n",
        "    \n",
        "    #return vid_id,title, transcript_list.find_transcript(['en']).fetch()\n",
        "    return vid_id,title,url\n",
        "\n",
        "\n",
        "def fetch_transcript_gen(vid_ids: Iterable):\n",
        "    for video_id in vid_ids:\n",
        "        yield fetch_transcript(video_id)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    from pprint import pprint\n",
        "#    for vid_id,title_, data_ in fetch_transcript_gen(['UyXJva9vg9o','6ZZONMmn3os']):\n",
        "videoListName = \"/home/ubuntu/you_tube_video_ids.txt\"\n",
        "with open(videoListName) as f:\n",
        "    video_ids = f.read().splitlines()\n",
        "    for vid_id,title_,url in fetch_transcript_gen(video_ids):\n",
        "        #print(f\"vid_id: {vid_id}\\tTitle: {title_}\\t Data length: {len(data_)}\\n\")\n",
        "        #print(f\"\\tTitle: {title_}\\turl:https://www.youtube.com/watch?v={vid_id}\")\n",
        "        print(f\"{vid_id}\\t{title_}|https://www.youtube.com/watch?v={vid_id}\")\n",
        "        #pprint(data_)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Dd2a74DZgEy8"
      },
      "id": "Dd2a74DZgEy8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Once We have the Video Id and the Title with youtube Link, we are ready to call the Pre-trained Model to Embed the data and then run queires to get the results performing Knn. Below is an example**"
      ],
      "metadata": {
        "id": "X17XkepQhBNS"
      },
      "id": "X17XkepQhBNS"
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "import os\n",
        "import csv\n",
        "import pickle\n",
        "import time\n",
        "import hnswlib\n",
        "\n",
        "##IMPORTING THE MODEL TO EMBED OUR DATA####\n",
        "\n",
        "model_name = 'quora-distilbert-multilingual'\n",
        "model = SentenceTransformer(model_name)\n",
        "\n",
        "dataset_path=\"/home/ubuntu/youtube_video_list.tsv\"\n",
        "\n",
        "##As the Dataset is Small I have added 93 as the Corpus Size\n",
        "max_corpus_size = 93\n",
        "\n",
        "embedding_cache_path = 'youtube-embeddings-{}-size-{}.pkl'.format(model_name.replace('/', '_'), max_corpus_size)\n",
        "\n",
        "\n",
        "embedding_size = 100    #Size of embeddings\n",
        "top_k_hits = 3         #Pulling only First 3 hits\n",
        "\n",
        "#Check if embedding cache path exists\n",
        "if not os.path.exists(embedding_cache_path):\n",
        "    # Check if the dataset exists. If not, download and extract\n",
        "    # Get all unique sentences from the file\n",
        "    corpus_sentences = set()\n",
        "    \n",
        "    with open(dataset_path, encoding='utf8') as fIn:\n",
        "        reader = csv.DictReader(fIn, delimiter='\\t', quoting=csv.QUOTE_MINIMAL)\n",
        "        for row in reader:\n",
        "            #print(row)\n",
        "            corpus_sentences.add(row['Youtube_title'])\n",
        "            if len(corpus_sentences) >= max_corpus_size:\n",
        "                break\n",
        "\n",
        "    corpus_sentences = list(corpus_sentences)\n",
        "    print(\"Encode the corpus. This might take a while\")\n",
        "    ###Creating EMbeddings and storing the file on local disc\n",
        "    corpus_embeddings = model.encode(corpus_sentences, show_progress_bar=True, convert_to_numpy=True)\n",
        "    print(\"Store file on disc\")\n",
        "    with open(embedding_cache_path, \"wb\") as fOut:\n",
        "        pickle.dump({'sentences': corpus_sentences, 'embeddings': corpus_embeddings}, fOut)\n",
        "else:\n",
        "    print(\"Load pre-computed embeddings from disc\")\n",
        "    with open(embedding_cache_path, \"rb\") as fIn:\n",
        "        cache_data = pickle.load(fIn)\n",
        "        corpus_sentences = cache_data['sentences']\n",
        "        corpus_embeddings = cache_data['embeddings']\n",
        "\n",
        "#Defining our hnswlib index\n",
        "index_path = \"./hnswlib.index\"\n",
        "#We use Inner Product (dot-product) as Index. We will normalize our vectors to unit length, then is Inner Product equal to cosine similarity\n",
        "index = hnswlib.Index(space = 'cosine', dim = embedding_size)\n",
        "\n",
        "if os.path.exists(index_path):\n",
        "    print(\"Loading index...\")\n",
        "    index.load_index(index_path)\n",
        "else:\n",
        "    ### Create the HNSWLIB index\n",
        "    print(\"Start creating HNSWLIB index\")\n",
        "    index.init_index(max_elements = len(corpus_embeddings), ef_construction = 400, M = 64)\n",
        "\n",
        "    # Then we train the index to find a suitable clustering\n",
        "    index.add_items(corpus_embeddings, list(range(len(corpus_embeddings))))\n",
        "\n",
        "    print(\"Saving index to:\", index_path)\n",
        "    index.save_index(index_path)\n",
        "\n",
        "# Controlling the recall by setting ef:\n",
        "index.set_ef(50)  # ef should always be > top_k_hits\n",
        "\n",
        "######### Search in the index ###########\n",
        "\n",
        "print(\"Corpus loaded with {} sentences / embeddings\".format(len(corpus_sentences)))\n",
        "\n",
        "while True:\n",
        "    inp_question = input(\"Please enter a question: \")\n",
        "\n",
        "    start_time = time.time()\n",
        "    question_embedding = model.encode(inp_question)\n",
        "\n",
        "    #We use hnswlib knn_query method to find the top_k_hits\n",
        "    corpus_ids, distances = index.knn_query(question_embedding, k=top_k_hits)\n",
        "\n",
        "    # We extract corpus ids and scores for the first query\n",
        "    hits = [{'corpus_id': id, 'score': 1-score} for id, score in zip(corpus_ids[0], distances[0])]\n",
        "    hits = sorted(hits, key=lambda x: x['score'], reverse=True)\n",
        "    end_time = time.time()\n",
        "\n",
        "    print(\"Input question:\", inp_question)\n",
        "    print(\"Results (after {:.3f} seconds):\".format(end_time-start_time))\n",
        "\n",
        "    # Approximate Nearest Neighbor (ANN) search\n",
        "    correct_hits = util.semantic_search(question_embedding, corpus_embeddings, top_k=top_k_hits)[0]\n",
        "    correct_hits_ids = set([hit['corpus_id'] for hit in correct_hits])\n",
        "    \n",
        "    print(\"correct hits\", correct_hits)\n",
        "    print(\"correct_hits_ids\", correct_hits_ids)\n",
        "\n",
        "    knn_corpus_ids = set([hit['corpus_id'] for hit in hits])\n",
        "        \n",
        "    if len(knn_corpus_ids) != len(correct_hits_ids):\n",
        "        print(\"Approximate Nearest Neighbor returned a different number of results than expected\")\n",
        "\n",
        "    recall = len(knn_corpus_ids.intersection(correct_hits_ids)) / len(correct_hits_ids)\n",
        "    \n",
        "    print(\"Results:\")\n",
        "    for hit in correct_hits[0:top_k_hits]:\n",
        "        if hit['corpus_id'] not in knn_corpus_ids:\n",
        "            print(\"\\t{:.3f}\\t{}\".format(hit['score'], corpus_sentences[hit['corpus_id']]))\n",
        "    print(\"\\n\\n========\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "i4x2ogeahUl0"
      },
      "id": "i4x2ogeahUl0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encode the corpus. This might take a while\n",
        "Batches: 100%\n",
        "3/3 [00:06<00:00, 1.94s/it]\n",
        "Store file on disc\n",
        "Loading index...\n",
        "Corpus loaded with 93 sentences / embeddings\n",
        "Please enter a question: how is Biden doing in his presidency\n",
        "Input question: how is Biden doing in his presidency\n",
        "Results (after 0.518 seconds):\n",
        "correct hits [{'corpus_id': 5, 'score': 0.8723465204238892}, {'corpus_id': 69, 'score': 0.8607958555221558}, {'corpus_id': 78, 'score': 0.8515018224716187}]\n",
        "correct_hits_ids {69, 5, 78}\n",
        "Results:\n",
        "\t0.872\tBiden adviser defends the president’s economy|https://www.youtube.com/watch?v=mKYi3TvX1U8\n",
        "\t0.861\tWhat's behind the Biden family's 'opulent' lifestyle?|https://www.youtube.com/watch?v=3OKIvDDNAC8\n",
        "\t0.852\tPerino: You have to wonder who the White House spin on 'recession' is for|https://www.youtube.com/watch?v=FrPEzSRhmEs\n",
        "\n",
        "\n",
        "========\n",
        "\n",
        "Please enter a question: Who are all in Senate\n",
        "Input question: Who are all in Senate\n",
        "Results (after 0.344 seconds):\n",
        "correct hits [{'corpus_id': 87, 'score': 0.8043516278266907}, {'corpus_id': 74, 'score': 0.780350923538208}, {'corpus_id': 79, 'score': 0.767914891242981}]\n",
        "correct_hits_ids {74, 79, 87}\n",
        "Results:\n",
        "\t0.804\tHouse Minority Leader McCarthy holds press conference|https://www.youtube.com/watch?v=PczZ6mI7390\n",
        "\t0.780\tKarine Jean-Pierre holds a White House briefing | 7/29/22|https://www.youtube.com/watch?v=-vI5kRdne1Q\n",
        "\t0.768\tBiden admin. puts politics above the American public: National Border Patrol Council VP|https://www.youtube.com/watch?v=2OxlSY0Mx40\n",
        "\n",
        "\n",
        "========\n",
        "\n",
        "Please enter a question: Which Party does Biden Belong to\n",
        "Input question: Which Party does Biden Belong to\n",
        "Results (after 0.333 seconds):\n",
        "correct hits [{'corpus_id': 69, 'score': 0.8159942626953125}, {'corpus_id': 71, 'score': 0.8026124835014343}, {'corpus_id': 16, 'score': 0.7910526394844055}]\n",
        "correct_hits_ids {16, 69, 71}\n",
        "Results:\n",
        "\t0.816\tWhat's behind the Biden family's 'opulent' lifestyle?|https://www.youtube.com/watch?v=3OKIvDDNAC8\n",
        "\t0.803\tBiden family 'too big to touch'|https://www.youtube.com/watch?v=_cpIbRimgOI\n",
        "\t0.791\t'The Five' on the media claiming Biden is 'back in the game'|https://www.youtube.com/watch?v=4cx4BJSm0Zc\n",
        "\n",
        "\n",
        "========\n",
        "\n",
        "Please enter a question: How Far is china?\n",
        "Input question: How Far is china?\n",
        "Results (after 0.323 seconds):\n",
        "correct hits [{'corpus_id': 41, 'score': 0.8983137011528015}, {'corpus_id': 27, 'score': 0.8780466318130493}, {'corpus_id': 15, 'score': 0.8729186654090881}]\n",
        "correct_hits_ids {41, 27, 15}\n",
        "Results:\n",
        "\t0.898\tTensions with China escalate as Pelosi visits Asia|https://www.youtube.com/watch?v=NsCb1-_a8_E\n",
        "\t0.878\tBiden threatened by China over Pelosi's Taiwan plans|https://www.youtube.com/watch?v=8wQOOZVTJ7o\n",
        "\t0.873\tChina issued an 'unacceptable threat' on Pelosi's trip, Biden once again not being clear: Waltz|https://www.youtube.com/watch?v=w8Hr91l7AM0\n",
        "\n",
        "\n",
        "========"
      ],
      "metadata": {
        "id": "o-l4--YLj1Cz"
      },
      "id": "o-l4--YLj1Cz"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "name": "Data_Mining_Final.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}